{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\muhammf\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\statsmodels\\tools\\_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
      "  import pandas.util.testing as tm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WESAD/S2/S2.pkl\n",
      "0\n",
      "500\n",
      "1000\n",
      "1500\n",
      "2000\n",
      "2500\n",
      "3000\n",
      "3500\n",
      "4000\n",
      "4500\n",
      "5000\n",
      "5500\n",
      "6000\n",
      "6500\n",
      "7000\n",
      "7500\n",
      "WESAD/S3/S3.pkl\n",
      "0\n",
      "500\n",
      "1000\n",
      "1500\n",
      "2000\n",
      "2500\n",
      "3000\n",
      "3500\n",
      "4000\n",
      "4500\n",
      "5000\n",
      "5500\n",
      "6000\n",
      "6500\n",
      "7000\n",
      "7500\n",
      "WESAD/S4/S4.pkl\n",
      "0\n",
      "500\n",
      "1000\n",
      "1500\n",
      "2000\n",
      "2500\n",
      "3000\n",
      "3500\n",
      "4000\n",
      "4500\n",
      "5000\n",
      "5500\n",
      "6000\n",
      "6500\n",
      "7000\n",
      "7500\n",
      "WESAD/S5/S5.pkl\n",
      "0\n",
      "500\n",
      "1000\n",
      "1500\n",
      "2000\n",
      "2500\n",
      "3000\n",
      "3500\n",
      "4000\n",
      "4500\n",
      "5000\n",
      "5500\n",
      "6000\n",
      "6500\n",
      "7000\n",
      "7500\n",
      "8000\n",
      "WESAD/S6/S6.pkl\n",
      "0\n",
      "500\n",
      "1000\n",
      "1500\n",
      "2000\n",
      "2500\n",
      "3000\n",
      "3500\n",
      "4000\n",
      "4500\n",
      "5000\n",
      "5500\n",
      "6000\n",
      "6500\n",
      "7000\n",
      "7500\n",
      "8000\n",
      "WESAD/S7/S7.pkl\n",
      "0\n",
      "500\n",
      "1000\n",
      "1500\n",
      "2000\n",
      "2500\n",
      "3000\n",
      "3500\n",
      "4000\n",
      "4500\n",
      "5000\n",
      "5500\n",
      "6000\n",
      "6500\n",
      "7000\n",
      "7500\n",
      "8000\n",
      "WESAD/S8/S8.pkl\n",
      "0\n",
      "500\n",
      "1000\n",
      "1500\n",
      "2000\n",
      "2500\n",
      "3000\n",
      "3500\n",
      "4000\n",
      "4500\n",
      "5000\n",
      "5500\n",
      "6000\n",
      "6500\n",
      "7000\n",
      "7500\n",
      "8000\n",
      "WESAD/S9/S9.pkl\n",
      "0\n",
      "500\n",
      "1000\n",
      "1500\n",
      "2000\n",
      "2500\n",
      "3000\n",
      "3500\n",
      "4000\n",
      "4500\n",
      "5000\n",
      "5500\n",
      "6000\n",
      "6500\n",
      "7000\n",
      "7500\n",
      "8000\n",
      "WESAD/S10/S10.pkl\n",
      "0\n",
      "500\n",
      "1000\n",
      "1500\n",
      "2000\n",
      "2500\n",
      "3000\n",
      "3500\n",
      "4000\n",
      "4500\n",
      "5000\n",
      "5500\n",
      "6000\n",
      "6500\n",
      "7000\n",
      "7500\n",
      "8000\n",
      "WESAD/S11/S11.pkl\n",
      "0\n",
      "500\n",
      "1000\n",
      "1500\n",
      "2000\n",
      "2500\n",
      "3000\n",
      "3500\n",
      "4000\n",
      "4500\n",
      "5000\n",
      "5500\n",
      "6000\n",
      "6500\n",
      "7000\n",
      "7500\n",
      "8000\n",
      "WESAD/S13/S13.pkl\n",
      "0\n",
      "500\n",
      "1000\n",
      "1500\n",
      "2000\n",
      "2500\n",
      "3000\n",
      "3500\n",
      "4000\n",
      "4500\n",
      "5000\n",
      "5500\n",
      "6000\n",
      "6500\n",
      "7000\n",
      "7500\n",
      "8000\n",
      "WESAD/S14/S14.pkl\n",
      "0\n",
      "500\n",
      "1000\n",
      "1500\n",
      "2000\n",
      "2500\n",
      "3000\n",
      "3500\n",
      "4000\n",
      "4500\n",
      "5000\n",
      "5500\n",
      "6000\n",
      "6500\n",
      "7000\n",
      "7500\n",
      "8000\n",
      "WESAD/S15/S15.pkl\n",
      "0\n",
      "500\n",
      "1000\n",
      "1500\n",
      "2000\n",
      "2500\n",
      "3000\n",
      "3500\n",
      "4000\n",
      "4500\n",
      "5000\n",
      "5500\n",
      "6000\n",
      "6500\n",
      "7000\n",
      "7500\n",
      "8000\n",
      "WESAD/S16/S16.pkl\n",
      "0\n",
      "500\n",
      "1000\n",
      "1500\n",
      "2000\n",
      "2500\n",
      "3000\n",
      "3500\n",
      "4000\n",
      "4500\n",
      "5000\n",
      "5500\n",
      "6000\n",
      "6500\n",
      "7000\n",
      "7500\n",
      "8000\n",
      "WESAD/S17/S17.pkl\n",
      "0\n",
      "500\n",
      "1000\n",
      "1500\n",
      "2000\n",
      "2500\n",
      "3000\n",
      "3500\n",
      "4000\n",
      "4500\n",
      "5000\n",
      "5500\n",
      "6000\n",
      "6500\n",
      "7000\n",
      "7500\n",
      "8000\n"
     ]
    }
   ],
   "source": [
    "#%matplotlib inline\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import csv\n",
    "import math\n",
    "from statistics import mode\n",
    "from scipy import stats\n",
    "import pywt\n",
    "from biosppy.signals import ecg, tools\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#from sklearn.datasets import fetch_mldata\n",
    "import seaborn as sns\n",
    "import sklearn as sklearn\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import sklearn.linear_model as skl_lm\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.tree import DecisionTreeClassifier # Import Decision Tree Classifier\n",
    "\n",
    "import graphviz \n",
    "import statsmodels.api as sm\n",
    "from sklearn import tree\n",
    "import xgboost as xgb\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def statistics(data): # The function for the 4 statistics\n",
    "    avg = np.mean(data) # mean\n",
    "    sd = np.std(data) # standard deviation\n",
    "    maxm = max(data) # maximum\n",
    "    minm = min(data) # minimum\n",
    "    s_mean, s_med, s_max, s_var, s_std_dev, s_abs_dev, s_kurtois, s_skew = tools.signal_stats(data)\n",
    "    return avg, sd, maxm, minm, s_med, s_max, s_var, s_abs_dev, s_kurtois, s_skew\n",
    "    \n",
    "def Derivatives(data): # Get the first and second derivatives of the data\n",
    "    deriv = (data[1:-1] + data[2:])/ 2. - (data[1:-1] + data[:-2])/ 2.\n",
    "    secondDeriv = data[2:] - 2*data[1:-1] + data[:-2]\n",
    "    return deriv,secondDeriv\n",
    "\n",
    "def strided_app(a, L, S ):  # Window len = L, Stride len/stepsize = S\n",
    "    nrows = ((a.size-L)//S)+1\n",
    "    n = a.strides[0]\n",
    "    return np.lib.stride_tricks.as_strided(a, shape=(nrows,L), strides=(S*n,n))\n",
    "\n",
    "\n",
    "data_set = 'WESAD/'\n",
    "\n",
    "s2_path = data_set + 'S2/S2.pkl'\n",
    "s3_path = data_set + 'S3/S3.pkl'\n",
    "s4_path = data_set + 'S4/S4.pkl'\n",
    "s5_path = data_set + 'S5/S5.pkl'\n",
    "s6_path = data_set + 'S6/S6.pkl'\n",
    "s7_path = data_set + 'S7/S7.pkl'\n",
    "s8_path = data_set + 'S8/S8.pkl'\n",
    "s9_path = data_set + 'S9/S9.pkl'\n",
    "s10_path = data_set + 'S10/S10.pkl'\n",
    "s11_path = data_set + 'S11/S11.pkl'\n",
    "s13_path = data_set + 'S13/S13.pkl'\n",
    "s14_path = data_set + 'S14/S14.pkl'\n",
    "s15_path = data_set + 'S15/S15.pkl'\n",
    "s16_path = data_set + 'S16/S16.pkl'\n",
    "s17_path = data_set + 'S17/S17.pkl'\n",
    "\n",
    "all_path = [s2_path, s3_path,s4_path,s5_path,s6_path,s7_path,s8_path,s9_path,s10_path,s11_path,s13_path,s14_path,s15_path,s16_path,s17_path]\n",
    "all_features = []\n",
    "all_label = []\n",
    "for subject_path in all_path:\n",
    "    print(subject_path)\n",
    "    with open(subject_path, 'rb') as file:\n",
    "        s2_data = pickle.load(file, encoding='latin1')\n",
    "        \n",
    "    label_1 = np.where(s2_data['label'] == 1)\n",
    "    start_1 = math.ceil(label_1[0][0]/175)\n",
    "    end_1 = math.floor(label_1[0][len(label_1[0])-1] / 175)\n",
    "\n",
    "    label_2 = np.where(s2_data['label'] == 2)\n",
    "    start_2 = math.ceil(label_2[0][0]/175)\n",
    "    end_2 = math.floor(label_2[0][len(label_2[0])-1] / 175)\n",
    "\n",
    "    label_3 = np.where(s2_data['label'] == 3)\n",
    "    start_3 = math.ceil(label_3[0][0]/175)\n",
    "    end_3 = math.floor(label_3[0][len(label_3[0])-1] / 175)\n",
    "\n",
    "    eda_data = s2_data['signal']['wrist']['EDA'][:,0]\n",
    "    acc_data = s2_data['signal']['wrist']['ACC']\n",
    "    bvp_data = s2_data['signal']['wrist']['BVP'][:,0]\n",
    "    temp_data = s2_data['signal']['wrist']['TEMP'][:,0]\n",
    "\n",
    "    eda_data_1 = eda_data[start_1:end_1+1]\n",
    "    eda_data_2 = eda_data[start_2:end_2+1]\n",
    "    eda_data_3 = eda_data[start_3:end_3+1]\n",
    "\n",
    "    acc_data_1 = acc_data[start_1*8:end_1*8+8,:]\n",
    "    acc_data_2 = acc_data[start_2*8:end_2*8+8,:]\n",
    "    acc_data_3 = acc_data[start_3*8:end_3*8+8,:]\n",
    "\n",
    "    bvp_data_1 = bvp_data[start_1*16:end_1*16+16]\n",
    "    bvp_data_2 = bvp_data[start_2*16:end_2*16+16]\n",
    "    bvp_data_3 = bvp_data[start_3*16:end_3*16+16]\n",
    "\n",
    "    temp_data_1 = temp_data[start_1*1:end_1*1+1]\n",
    "    temp_data_2 = temp_data[start_2*1:end_2*1+1]\n",
    "    temp_data_3 = temp_data[start_3*1:end_3*1+1]\n",
    "\n",
    "    EDA_1 = strided_app(eda_data_1, 240, 1) # Window len = L, Stride len/stepsize = S\n",
    "    EDA_2 = strided_app(eda_data_2, 240, 1)\n",
    "    EDA_3 = strided_app(eda_data_3, 240, 1)\n",
    "\n",
    "    ACCx_1 = strided_app(acc_data_1[:,0], 1920, 8) # Window len = L, Stride len/stepsize = S\n",
    "    ACCy_1 = strided_app(acc_data_1[:,1], 1920, 8)\n",
    "    ACCz_1 = strided_app(acc_data_1[:,2], 1920, 8)\n",
    "    ACCx_2 = strided_app(acc_data_2[:,0], 1920, 8)\n",
    "    ACCy_2 = strided_app(acc_data_2[:,1], 1920, 8)\n",
    "    ACCz_2 = strided_app(acc_data_2[:,2], 1920, 8)\n",
    "    ACCx_3 = strided_app(acc_data_3[:,0], 1920, 8)\n",
    "    ACCy_3 = strided_app(acc_data_3[:,1], 1920, 8)\n",
    "    ACCz_3 = strided_app(acc_data_3[:,2], 1920, 8)\n",
    "\n",
    "    BVP_1 = strided_app(bvp_data_1, 3840, 16)\n",
    "    BVP_2 = strided_app(bvp_data_2, 3840, 16)\n",
    "    BVP_3 = strided_app(bvp_data_3, 3840, 16)\n",
    "\n",
    "    TEMP_1 = strided_app(temp_data_1, 240, 1)\n",
    "    TEMP_2 = strided_app(temp_data_2, 240, 1)\n",
    "    TEMP_3 = strided_app(temp_data_3, 240, 1)\n",
    "\n",
    "    EDA = np.concatenate((EDA_1, EDA_2, EDA_3),axis=0) \n",
    "    ACCx = np.concatenate((ACCx_1, ACCx_2, ACCx_3),axis=0) \n",
    "    ACCy = np.concatenate((ACCy_1, ACCy_2, ACCy_3),axis=0) \n",
    "    ACCz = np.concatenate((ACCz_1, ACCz_2, ACCz_3),axis=0)\n",
    "    \n",
    "    ACC = np.sqrt(np.square(ACCx) + np.square(ACCy) + np.square(ACCz))\n",
    "    \n",
    "    BVP = np.concatenate((BVP_1, BVP_2, BVP_3),axis=0) \n",
    "\n",
    "    smooth_signal = tools.smoother(BVP, kernel='median', size=5) # Convolutional 5x5 kernel window\n",
    "    data = smooth_signal['signal']\n",
    "\n",
    "    norm_signal = tools.normalize(data)\n",
    "    BVP = norm_signal['signal']\n",
    "\n",
    "    TEMP = np.concatenate((TEMP_1, TEMP_2, TEMP_3),axis=0) \n",
    "\n",
    "    label_1 = [1] * len(EDA_1)\n",
    "    label_2 = [2] * len(EDA_2)\n",
    "    label_3 = [1] * len(EDA_3)\n",
    "\n",
    "    LABEL = np.concatenate((label_1, label_2, label_3),axis=0) \n",
    "    \n",
    "    all_label.append(LABEL)\n",
    "\n",
    "    # Construct the feature matrix, 60 EDA features, 240 ACC features, 60 TEMP features, 60 BVA features, and 420 features in total. \n",
    "    \n",
    "    length=len(LABEL)\n",
    "    features = np.zeros((length,420))\n",
    "\n",
    "    for i in range(length):\n",
    "        if(i%500==0):\n",
    "            print(i)\n",
    "        deriv_EDA,secondDeriv_EDA = Derivatives(EDA[i,:])\n",
    "        deriv_ACC,secondDeriv_ACC = Derivatives(ACC[i,:])\n",
    "        deriv_ACCx,secondDeriv_ACCx = Derivatives(ACCx[i,:])\n",
    "        deriv_ACCy,secondDeriv_ACCy = Derivatives(ACCy[i,:])\n",
    "        deriv_ACCz,secondDeriv_ACCz = Derivatives(ACCz[i,:])\n",
    "        _, EDA_cD_3, EDA_cD_2, EDA_cD_1 = pywt.wavedec(EDA[i,:], 'Haar', level=3) #3 = 1Hz, 2 = 2Hz, 1=4Hz\n",
    "        _, ACC_cD_3, ACC_cD_2, ACC_cD_1 = pywt.wavedec(ACC[i,:], 'Haar', level=3) \n",
    "        _, ACCx_cD_3, ACCx_cD_2, ACCx_cD_1 = pywt.wavedec(ACCx[i,:], 'Haar', level=3) \n",
    "        _, ACCy_cD_3, ACCy_cD_2, ACCy_cD_1 = pywt.wavedec(ACCy[i,:], 'Haar', level=3) \n",
    "        _, ACCz_cD_3, ACCz_cD_2, ACCz_cD_1 = pywt.wavedec(ACCz[i,:], 'Haar', level=3) \n",
    "\n",
    "        deriv_TEMP,secondDeriv_TEMP = Derivatives(TEMP[i,:])\n",
    "        _, TEMP_cD_3, TEMP_cD_2, TEMP_cD_1 = pywt.wavedec(TEMP[i,:], 'Haar', level=3)\n",
    "        \n",
    "        deriv_BVP,secondDeriv_BVP = Derivatives(BVP[i,:])\n",
    "        _, BVP_cD_3, BVP_cD_2, BVP_cD_1 = pywt.wavedec(BVP[i,:], 'Haar', level=3)\n",
    "        \n",
    "\n",
    "        ### EDA features\n",
    "        # EDA statistical features:\n",
    "        features[i,0:10] = statistics(EDA[i,:])\n",
    "        features[i,10:20] = statistics(deriv_EDA)\n",
    "        features[i,20:30] = statistics(secondDeriv_EDA)\n",
    "        # EDA wavelet features:\n",
    "        features[i,30:40] = statistics(EDA_cD_3)\n",
    "        features[i,40:50] = statistics(EDA_cD_2)\n",
    "        features[i,50:60] = statistics(EDA_cD_1)\n",
    "\n",
    "        ### ACC features\n",
    "        ## ACC statistical features:\n",
    "        # Acceleration magnitude:\n",
    "        features[i,60:70] = statistics(ACC[i,:])\n",
    "        features[i,70:80] = statistics(deriv_ACC)\n",
    "        features[i,80:90] = statistics(secondDeriv_ACC)\n",
    "        # Acceleration x-axis:\n",
    "        features[i,90:100] = statistics(ACCx[i,:])\n",
    "        features[i,100:110] = statistics(deriv_ACCx)\n",
    "        features[i,110:120] = statistics(secondDeriv_ACCx)\n",
    "        # Acceleration y-axis:\n",
    "        features[i,120:130] = statistics(ACCy[i,:])\n",
    "        features[i,130:140] = statistics(deriv_ACCy)\n",
    "        features[i,140:150] = statistics(secondDeriv_ACCy)\n",
    "        # Acceleration z-axis:\n",
    "        features[i,150:160] = statistics(ACCz[i,:])\n",
    "        features[i,160:170] = statistics(deriv_ACCz)\n",
    "        features[i,170:180] = statistics(secondDeriv_ACCz)\n",
    "        ## ACC wavelet features:\n",
    "        # ACC magnitude wavelet features:\n",
    "        features[i,180:190] = statistics(ACC_cD_3)\n",
    "        features[i,190:200] = statistics(ACC_cD_2)\n",
    "        features[i,200:210] = statistics(ACC_cD_1)\n",
    "        # ACC x-axis wavelet features:\n",
    "        features[i,210:220] = statistics(ACCx_cD_3)\n",
    "        features[i,220:230] = statistics(ACCx_cD_2)\n",
    "        features[i,230:240] = statistics(ACCx_cD_1)\n",
    "        # ACC y-axis wavelet features:\n",
    "        features[i,240:250] = statistics(ACCy_cD_3)\n",
    "        features[i,250:260] = statistics(ACCy_cD_2)\n",
    "        features[i,260:270] = statistics(ACCy_cD_1)\n",
    "        # ACC z-axis wavelet features:\n",
    "        features[i,270:280] = statistics(ACCz_cD_3)\n",
    "        features[i,280:290] = statistics(ACCz_cD_2)\n",
    "        features[i,290:300] = statistics(ACCz_cD_1)\n",
    "\n",
    "        ### TEMP features\n",
    "        # TEMP statistical features:\n",
    "        features[i,300:310] = statistics(TEMP[i,:])\n",
    "        features[i,310:320] = statistics(deriv_TEMP)\n",
    "        features[i,320:330] = statistics(secondDeriv_TEMP)\n",
    "        # TEMP wavelet features:\n",
    "        features[i,330:340] = statistics(TEMP_cD_3)\n",
    "        features[i,340:350] = statistics(TEMP_cD_2)\n",
    "        features[i,350:360] = statistics(TEMP_cD_1)\n",
    "        \n",
    "         ### BVP features\n",
    "        # BVP statistical features:\n",
    "        features[i,360:370] = statistics(BVP[i,:])\n",
    "        features[i,370:380] = statistics(deriv_BVP)\n",
    "        features[i,380:390] = statistics(secondDeriv_BVP)\n",
    "        # BVP wavelet features:\n",
    "        features[i,390:400] = statistics(BVP_cD_3)\n",
    "        features[i,400:410] = statistics(BVP_cD_2)\n",
    "        features[i,410:420] = statistics(BVP_cD_1)\n",
    "\n",
    "        \n",
    "    all_features.append(features)\n",
    "\n",
    "file_to_store = open(\"all_featuresv2.pickle\", \"wb\")\n",
    "pickle.dump(all_features, file_to_store)\n",
    "file_to_store.close()\n",
    "\n",
    "file_to_store = open(\"all_labelv2.pickle\", \"wb\")\n",
    "pickle.dump(all_label, file_to_store)\n",
    "file_to_store.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
